<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Will Ye</title>
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1"
    />
    <link rel="stylesheet" href="https://will-ye.com/style.css">
    <script>!function(){var e=window.Cohere=window.Cohere||[];if(e.invoked)console.error("Tried to load Cohere twice");else{e.invoked=!0,e.snippet="0.2",e.methods=["init","identify","stop","showCode","getSessionUrl","makeCall","addCallStatusListener","removeCallStatusListener","widget","addSessionUrlListener","removeSessionUrlListener",],e.methods.forEach(function(o){e[o]=function(){var t=Array.prototype.slice.call(arguments);t.unshift(o),e.push(t)}});var o=document.createElement("script");o.type="text/javascript",o.async=!0,o.src="https://static.cohere.so/main.js",o.crossOrigin="anonymous";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(o,t)}}();</script>
    <script>window.Cohere.init("HmoLpmLAXQxi7HwVR092zClX");</script>

    <!-- Big thanks to this guy: https://louveta.gitlab.io/blog/katex/ -->
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" 
    integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" 
    integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/mathtex-script-type.min.js" 
    integrity="sha384-zWYbd0NBwgTsgIdFKVprSfTh1mbMPe5Hz1X3yY4Sd1h/K1cQoUe36OGwAGz/PcDy" crossorigin="anonymous"></script>
        
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" 
    integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        
    
    

    
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" 
        integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
        
    

    
    <meta name="title" content="A simple performance optimization for softmax, explained">
    
    
  </head>

  <body>
    <section class="section">
      <div class="container">
<div class="post-nav">
  <a href="/">Back</a>
</div>
<div class="post-header">
  <h1 class="post-title">A simple performance optimization for softmax, explained</h1>
  
    <p class="post-meta">November 25, 2025</p>
  
</div>
<div class="post-content"><p>The softmax function is used frequently in neural networks, such as within large language models like ChatGPT. It converts a list of numbers into a probability distribution, with the values adding up to 1.</p>
<script type="math/tex;mode=display">\begin{aligned}
x &= [5,\;7,\;8] \\
\mathrm{softmax}(x) &\approx [0.035,\; 0.259,\; 0.706]
\end{aligned}</script>
<p>Softmax does this by exponentiating each value and then dividing by the sum of all the exponentiated values.</p>
<script type="math/tex;mode=display">\begin{aligned}
\mathrm{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}
\end{aligned}</script>
<p>Using the previous example, here is how the softmax was computed:</p>
<script type="math/tex;mode=display">\begin{aligned}
x &= [5,\;7,\;8] \\
\mathrm{softmax}(x) &= \left[
\frac{e^5}{e^5 + e^7 + e^8},\;
\frac{e^7}{e^5 + e^7 + e^8},\;
\frac{e^8}{e^5 + e^7 + e^8}
\right]
\end{aligned}</script>
<p>Obviously, softmax uses many base \(e\) operations!</p>
<p>However, in real world implementations, <a href="https://x.com/cHHillee/status/1993024196872749339">softmax is modified to use base 2 instead of base \(e\)</a> because computers are much faster at computing base 2 operations compared to base \(e\):</p>
<blockquote>
<p>This is a standard optimization in FlashAttention, where the softmax exponents are a non-negligible cost of the inner loop. It rescales the inputs by 1/ln2 so that you can directly use exp2 instead of exp.</p>
</blockquote>
<p>So how do we convert the base \(e\) to base 2? We could just use the exponential base change formula<sup class="footnote-reference" id="fr-1-1"><a href="#fn-1">1</a></sup>, but it's fun (and satisfying) to derive it ourselves.</p>
<p>We want to find some expression \(y\) so that \(e^x = 2^y\). We'll take the natural log of both sides to cancel out the \(e\) and use the logarithmic power rule to bring the \(y\) out.</p>
<script type="math/tex;mode=display">\begin{aligned}
e^x &= 2^y \\
\ln(e^x) &= \ln(2^y) \\
x &= \ln(2^y) \\
x &= y \ln 2 \\
y &= \frac{x}{\ln 2}
\end{aligned}</script>
<p>We can now plug \(y\) back in:</p>
<script type="math/tex;mode=display">\begin{aligned}
e^x &= 2^y \\
e^x &= 2^{x/\ln 2}
\end{aligned}</script>
<p>So that's why FlashAttention scales inputs by \(1/\ln 2\)!</p>
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn-1">
<p>\(a^b = c^{b \log_c a}\) <a href="#fr-1-1">â†©</a></p>
</li>
</ol>
</section>
</div>
</div>
    </section>
  </body>
</html>
